/home/mathias/.venv/keras/bin/python /home/mathias/PycharmProjects/MotionClassifier/train_classifier.py
Using TensorFlow backend.
Train on 2340 samples, validate on 180 samples
Epoch 1/1
2017-10-15 11:06:21.754343: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-15 11:06:21.754364: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2017-10-15 11:06:21.754369: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
  60/2340 [..............................] - ETA: 68s - loss: 0.6924 - acc: 0.5133
 120/2340 [>.............................] - ETA: 57s - loss: 0.8096 - acc: 0.4308
 180/2340 [=>............................] - ETA: 52s - loss: 0.7777 - acc: 0.5150
 240/2340 [==>...........................] - ETA: 50s - loss: 0.7541 - acc: 0.5425
 300/2340 [==>...........................] - ETA: 48s - loss: 0.7415 - acc: 0.5433
 360/2340 [===>..........................] - ETA: 47s - loss: 0.7287 - acc: 0.5639
 420/2340 [====>.........................] - ETA: 46s - loss: 0.7169 - acc: 0.5688
 480/2340 [=====>........................] - ETA: 44s - loss: 0.7170 - acc: 0.5529
 540/2340 [=====>........................] - ETA: 44s - loss: 0.7075 - acc: 0.5513
 600/2340 [======>.......................] - ETA: 44s - loss: 0.7085 - acc: 0.5472
 660/2340 [=======>......................] - ETA: 42s - loss: 0.7040 - acc: 0.5498
 720/2340 [========>.....................] - ETA: 40s - loss: 0.7003 - acc: 0.5486
 780/2340 [=========>....................] - ETA: 39s - loss: 0.6964 - acc: 0.5528
 840/2340 [=========>....................] - ETA: 37s - loss: 0.6968 - acc: 0.5476
 900/2340 [==========>...................] - ETA: 36s - loss: 0.6932 - acc: 0.5552
 960/2340 [===========>..................] - ETA: 34s - loss: 0.6903 - acc: 0.5594
1020/2340 [============>.................] - ETA: 33s - loss: 0.6876 - acc: 0.5595
1080/2340 [============>.................] - ETA: 32s - loss: 0.6884 - acc: 0.5564
1140/2340 [=============>................] - ETA: 30s - loss: 0.6858 - acc: 0.5539
1200/2340 [==============>...............] - ETA: 29s - loss: 0.6843 - acc: 0.5548
1260/2340 [===============>..............] - ETA: 27s - loss: 0.6844 - acc: 0.5529
1320/2340 [===============>..............] - ETA: 25s - loss: 0.6817 - acc: 0.5524
1380/2340 [================>.............] - ETA: 24s - loss: 0.6767 - acc: 0.5538
1440/2340 [=================>............] - ETA: 22s - loss: 0.6768 - acc: 0.5490
1500/2340 [==================>...........] - ETA: 21s - loss: 0.6765 - acc: 0.5493
1560/2340 [===================>..........] - ETA: 19s - loss: 0.6766 - acc: 0.5502
1620/2340 [===================>..........] - ETA: 17s - loss: 0.6746 - acc: 0.5526
1680/2340 [====================>.........] - ETA: 16s - loss: 0.6733 - acc: 0.5515
1740/2340 [=====================>........] - ETA: 15s - loss: 0.6714 - acc: 0.5532
1800/2340 [======================>.......] - ETA: 13s - loss: 0.6714 - acc: 0.5513
1860/2340 [======================>.......] - ETA: 11s - loss: 0.6699 - acc: 0.5517
1920/2340 [=======================>......] - ETA: 10s - loss: 0.6689 - acc: 0.5518
1980/2340 [========================>.....] - ETA: 8s - loss: 0.6677 - acc: 0.5498
2040/2340 [=========================>....] - ETA: 7s - loss: 0.6665 - acc: 0.5520
2100/2340 [=========================>....] - ETA: 5s - loss: 0.6660 - acc: 0.5508
2160/2340 [==========================>...] - ETA: 4s - loss: 0.6664 - acc: 0.5486
2220/2340 [===========================>..] - ETA: 2s - loss: 0.6661 - acc: 0.5488
2280/2340 [============================>.] - ETA: 1s - loss: 0.6647 - acc: 0.5508
2340/2340 [==============================] - 59s - loss: 0.6651 - acc: 0.5489 - val_loss: 0.7329 - val_acc: 0.5806
_________________________________________________________________
Layer (type)                 Output Shape              Param #
=================================================================
gru_1 (GRU)                  (60, 10, 1024)            3173376
_________________________________________________________________
gru_2 (GRU)                  (60, 10, 1024)            6294528
_________________________________________________________________
dropout_1 (Dropout)          (60, 10, 1024)            0
_________________________________________________________________
dense_1 (Dense)              (60, 10, 2)               2050
=================================================================
Total params: 9,469,954
Trainable params: 9,469,954
Non-trainable params: 0
_________________________________________________________________
p vs y: [ 0.56532139  0.43467861] - [ 0.  1.] --> (abs) 0.56532138586
p vs y: [ 0.70050371  0.29949629] - [ 1.  0.] --> (abs) 0.299496293068*
p vs y: [ 0.5537281   0.44627196] - [ 0.  1.] --> (abs) 0.553728073835
p vs y: [ 0.54449165  0.45550832] - [ 1.  0.] --> (abs) 0.455508336425
p vs y: [ 0.57589847  0.42410153] - [ 0.  1.] --> (abs) 0.575898468494
p vs y: [ 0.62084067  0.37915933] - [ 1.  0.] --> (abs) 0.379159331322
p vs y: [ 0.59403878  0.40596122] - [ 0.  1.] --> (abs) 0.594038784504
p vs y: [ 0.61821949  0.38178053] - [ 1.  0.] --> (abs) 0.381780520082
p vs y: [ 0.88191909  0.11808084] - [ 0.  1.] --> (abs) 0.881919123232
p vs y: [ 0.53680336  0.46319667] - [ 1.  0.] --> (abs) 0.463196650147
p vs y: [ 0.54211187  0.45788813] - [ 0.  1.] --> (abs) 0.542111873627
p vs y: [ 0.53905785  0.46094209] - [ 1.  0.] --> (abs) 0.46094211936
p vs y: [ 0.54214203  0.457858  ] - [ 0.  1.] --> (abs) 0.542142018676
p vs y: [ 0.8726927   0.12730739] - [ 1.  0.] --> (abs) 0.127307340503*
p vs y: [ 0.59114683  0.40885323] - [ 0.  1.] --> (abs) 0.591146796942
p vs y: [ 0.57482576  0.4251743 ] - [ 1.  0.] --> (abs) 0.4251742661
p vs y: [ 0.57858312  0.42141691] - [ 0.  1.] --> (abs) 0.578583106399
p vs y: [  9.99999702e-01   3.37928128e-07] - [ 1.  0.] --> (abs) 3.17975676012e-07*
p vs y: [ 0.67766988  0.32233018] - [ 0.  1.] --> (abs) 0.677669852972
p vs y: [ 0.59650218  0.40349779] - [ 1.  0.] --> (abs) 0.403497800231
p vs y: [ 0.97958106  0.02041895] - [ 1.  0.] --> (abs) 0.0204189475626*
p vs y: [ 0.59696466  0.40303534] - [ 0.  1.] --> (abs) 0.596964657307
p vs y: [ 0.5551722  0.4448278] - [ 1.  0.] --> (abs) 0.444827795029
p vs y: [ 0.56857908  0.43142095] - [ 0.  1.] --> (abs) 0.568579062819
p vs y: [ 0.56874418  0.43125582] - [ 1.  0.] --> (abs) 0.431255817413
p vs y: [ 0.68083853  0.31916144] - [ 0.  1.] --> (abs) 0.680838540196
p vs y: [ 0.55712664  0.44287333] - [ 1.  0.] --> (abs) 0.442873343825
p vs y: [ 0.56074375  0.43925628] - [ 0.  1.] --> (abs) 0.560743734241
p vs y: [ 0.57067871  0.42932129] - [ 1.  0.] --> (abs) 0.429321289062
p vs y: [ 0.56280863  0.43719134] - [ 0.  1.] --> (abs) 0.562808647752
p vs y: [ 0.57776248  0.42223749] - [ 1.  0.] --> (abs) 0.422237500548
p vs y: [ 0.57489347  0.42510653] - [ 0.  1.] --> (abs) 0.574893474579
p vs y: [ 0.74419755  0.25580242] - [ 1.  0.] --> (abs) 0.255802437663*
p vs y: [ 0.57229167  0.42770833] - [ 0.  1.] --> (abs) 0.57229167223
p vs y: [ 0.71017748  0.28982258] - [ 1.  0.] --> (abs) 0.289822548628*
p vs y: [ 0.54082626  0.45917374] - [ 1.  0.] --> (abs) 0.459173738956
p vs y: [ 0.60784662  0.39215344] - [ 0.  1.] --> (abs) 0.607846587896
p vs y: [ 0.53555548  0.46444455] - [ 1.  0.] --> (abs) 0.46444453299
p vs y: [ 0.56672275  0.43327728] - [ 0.  1.] --> (abs) 0.566722735763
p vs y: [ 0.97192889  0.0280711 ] - [ 1.  0.] --> (abs) 0.0280711008236*
p vs y: [ 0.56960851  0.43039146] - [ 0.  1.] --> (abs) 0.569608524442
p vs y: [ 0.504345  0.495655] - [ 1.  0.] --> (abs) 0.49565500021
p vs y: [ 0.57280093  0.42719913] - [ 0.  1.] --> (abs) 0.572800904512
p vs y: [ 0.92299092  0.07700909] - [ 1.  0.] --> (abs) 0.0770090855658*
p vs y: [ 0.58418429  0.41581574] - [ 0.  1.] --> (abs) 0.584184274077
p vs y: [ 0.56866014  0.43133983] - [ 1.  0.] --> (abs) 0.431339845061
p vs y: [ 0.66599023  0.3340098 ] - [ 0.  1.] --> (abs) 0.66599021852
p vs y: [ 0.99738586  0.0026141 ] - [ 1.  0.] --> (abs) 0.00261411943939*
p vs y: [ 0.54360557  0.45639443] - [ 0.  1.] --> (abs) 0.543605566025
p vs y: [ 0.5756973   0.42430273] - [ 1.  0.] --> (abs) 0.424302712083
p vs y: [ 0.57521951  0.42478046] - [ 0.  1.] --> (abs) 0.575219526887
p vs y: [  9.99594986e-01   4.04952938e-04] - [ 1.  0.] --> (abs) 0.000404983249609*
p vs y: [ 0.5291487   0.47085133] - [ 0.  1.] --> (abs) 0.529148682952
p vs y: [ 0.68413925  0.31586075] - [ 1.  0.] --> (abs) 0.315860748291
p vs y: [ 0.56718946  0.43281054] - [ 0.  1.] --> (abs) 0.567189455032
p vs y: [ 0.61718339  0.38281658] - [ 1.  0.] --> (abs) 0.382816597819
p vs y: [ 0.6279915  0.3720085] - [ 0.  1.] --> (abs) 0.627991497517
p vs y: [ 0.56706995  0.43293005] - [ 1.  0.] --> (abs) 0.43293005228
p vs y: [ 0.69286209  0.30713791] - [ 0.  1.] --> (abs) 0.692862093449
p vs y: [ 0.60988986  0.39011008] - [ 1.  0.] --> (abs) 0.390110105276
Done!

Process finished with exit code 0
